{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Helper Functions for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from pycirclize import Circos\n",
    "from pycirclize.parser import Gff\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def count_variants(vcf_file):\n",
    "    \"\"\"Count the number of variants in a VCF file using subprocess.\"\"\"\n",
    "    if vcf_file.endswith('.gz'):\n",
    "        cmd = f\"bcftools view -H {vcf_file} | wc -l\"\n",
    "    else:\n",
    "        cmd = f\"grep -v '^#' {vcf_file} | wc -l\"\n",
    "    \n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return int(result.stdout.strip())\n",
    "\n",
    "def filter_variants():\n",
    "    # Define input and output VCF files\n",
    "    INPUT_VCF = \"../data/freebayes.annotated_pc1.vcf.gz\"\n",
    "    QUAL_FILTERED_VCF = \"../data/filtered_qual.vcf\"\n",
    "    DP_FILTERED_VCF = \"../data/filtered_dp.vcf\"\n",
    "    SNP_FILTERED_VCF = \"../data/filtered_snp.vcf\"\n",
    "    FINAL_VCF = \"../data/filtered_final.vcf\"\n",
    "    \n",
    "    print(\"======================================\")\n",
    "    print(\"Starting Variant Filtering Process\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    # Count initial number of variants\n",
    "    START_COUNT = count_variants(INPUT_VCF)\n",
    "    print(f\"Total variants before filtering: {START_COUNT}\")\n",
    "    \n",
    "    # Step 1: Filter out low-quality variants (QUAL < 30)\n",
    "    subprocess.run(f\"bcftools filter -e 'QUAL < 30' {INPUT_VCF} -o {QUAL_FILTERED_VCF}\", shell=True)\n",
    "    QUAL_FILTERED_COUNT = count_variants(QUAL_FILTERED_VCF)\n",
    "    print(f\"Stage 1: QUAL filtering: {START_COUNT - QUAL_FILTERED_COUNT} Variants removed and {QUAL_FILTERED_COUNT} variants left\")\n",
    "\n",
    "    # Step 2: Filter variants based on per-sample depth (FORMAT/DP < 10 or > 150)\n",
    "    subprocess.run(f\"bcftools view -i 'FMT/DP >= 30 & FMT/DP <= 150' {QUAL_FILTERED_VCF} -o {DP_FILTERED_VCF}\", shell=True)\n",
    "    DP_FILTERED_COUNT = count_variants(DP_FILTERED_VCF)\n",
    "    print(f\"Stage 2: FORMAT/DP filtering, DP >= 30 & DP <= 150: {QUAL_FILTERED_COUNT - DP_FILTERED_COUNT} Variants removed and {DP_FILTERED_COUNT} variants left\")\n",
    "\n",
    "    # Step 3: Retain SNPs and indels (Remove other variant types if any)\n",
    "    subprocess.run(f\"bcftools view -v snps,indels {DP_FILTERED_VCF} -o {SNP_FILTERED_VCF}\", shell=True)\n",
    "    SNP_FILTERED_COUNT = count_variants(SNP_FILTERED_VCF)\n",
    "    print(f\"Stage 3: After keeping SNPs and indels: {DP_FILTERED_COUNT - SNP_FILTERED_COUNT} Variants removed and {SNP_FILTERED_COUNT} variants left\")\n",
    "\n",
    "    # Rename final output\n",
    "    os.rename(SNP_FILTERED_VCF, FINAL_VCF)\n",
    "    FINAL_COUNT = count_variants(FINAL_VCF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_vcf(vcf_file):\n",
    "    \"\"\"Reads a VCF file, automatically detecting the header and using correct column names.\"\"\"\n",
    "    \n",
    "    # Find the header line dynamically\n",
    "    with open(vcf_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#CHROM\"):\n",
    "                header = line.strip().split(\"\\t\")  # Extract column names\n",
    "                break  # Stop searching after finding header\n",
    "    \n",
    "    # Read VCF using Pandas, skipping comment lines\n",
    "    df = pd.read_csv(vcf_file, sep=\"\\t\", comment=\"#\", header=None, names=header)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def find_index(format_value, field):\n",
    "    \"\"\"Finds the index of a specific field (e.g., RO, AO, DP) in the FORMAT column.\"\"\"\n",
    "    item_list = format_value.split(':')\n",
    "    return item_list.index(field) if field in item_list else None\n",
    "\n",
    "def expand_multiallelic_variants(df_vcf):\n",
    "    \"\"\"\n",
    "    Extracts allele counts (RO, AO, DP) for each sample from the VCF DataFrame\n",
    "    and expands multi-allelic variants into separate rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract sample names (everything after FORMAT column)\n",
    "    sample_names = df_vcf.columns[9:]  # Skip CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT\n",
    "\n",
    "    # Find the index positions of RO, AO, and DP in the FORMAT field\n",
    "    format_example = df_vcf['FORMAT'].values[0]  # Take the first row as an example\n",
    "    ro_index = find_index(format_example, 'RO')\n",
    "    ao_index = find_index(format_example, 'AO')\n",
    "    dp_index = find_index(format_example, 'DP')\n",
    "\n",
    "    # Ensure indices exist\n",
    "    if None in [ro_index, ao_index, dp_index]:\n",
    "        raise ValueError(\"RO, AO, or DP field not found in FORMAT column.\")\n",
    "\n",
    "    # Initialize an empty list to store expanded rows\n",
    "    expanded_rows = []\n",
    "\n",
    "    for _, row in df_vcf.iterrows():\n",
    "        # Split ALT alleles (multi-allelic sites will have multiple ALT values)\n",
    "        alt_alleles = row['ALT'].split(',')\n",
    "        info_type = row['INFO'].split('TYPE=')[1].split(';')[0].split(',')\n",
    "        \n",
    "        # Process each ALT allele separately\n",
    "        for i, (alt, inty )in enumerate(zip(alt_alleles,info_type)):\n",
    "            new_row = {\n",
    "                \"#CHROM\": row[\"#CHROM\"],\n",
    "                \"POS\": row[\"POS\"],\n",
    "                \"REF\": row[\"REF\"],\n",
    "                \"ALT\": alt,  # Assign each alternate allele to a separate row\n",
    "                \"INFO_TYPE\": inty\n",
    "            }\n",
    "\n",
    "            for sample in sample_names:\n",
    "                # Split FORMAT fields for the sample\n",
    "                sample_values = row[sample].split(':')\n",
    "                \n",
    "                # Extract and store RO and DP\n",
    "                new_row[f\"RO_{sample}\"] = int(sample_values[ro_index]) if sample_values[ro_index] != '.' else 0\n",
    "                new_row[f\"DP_{sample}\"] = int(sample_values[dp_index]) if sample_values[dp_index] != '.' else 0\n",
    "\n",
    "                # Extract AO for the specific ALT allele\n",
    "                ao_values = sample_values[ao_index].split(',')  # Multiple values for multiple ALT alleles\n",
    "                new_row[f\"AO_{sample}\"] = int(ao_values[i]) if i < len(ao_values) and ao_values[i] != '.' else 0\n",
    "\n",
    "            # Append expanded row\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    # Convert list of dictionaries into DataFrame\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_frequencies(df_counts):\n",
    "    \"\"\"\n",
    "    Computes allele frequency (AF = AO / DP) for each sample in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract sample names from AO columns\n",
    "    sample_names = [col.replace(\"AO_\", \"\") for col in df_counts.columns if col.startswith(\"AO_\")]\n",
    "\n",
    "    # Create a new DataFrame to store allele frequencies\n",
    "    df_af = df_counts[['#CHROM', 'POS', 'REF', 'ALT']].copy()\n",
    "\n",
    "    for sample in sample_names:\n",
    "        ao_col = f\"AO_{sample}\"\n",
    "        dp_col = f\"DP_{sample}\"\n",
    "        af_col = f\"AF_{sample}\"\n",
    "        df_af[af_col] = df_counts[ao_col] / df_counts[dp_col] #(df_counts[ro_col] + df_counts[ao_col])\n",
    "\n",
    "    return df_af\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_first_ann(info_field):\n",
    "    \"\"\"\n",
    "    Extract the first ANN annotation from a VCF INFO field.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    info_field : str\n",
    "        The INFO field from a VCF file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing the variant type, impact, and gene ID,\n",
    "        or None if no ANN field is found\n",
    "    \"\"\"\n",
    "    # Check if there's an ANN field\n",
    "    if 'ANN=' not in info_field:\n",
    "        return None\n",
    "    \n",
    "    # Extract the ANN part\n",
    "    ann_start = info_field.find('ANN=')\n",
    "    # Take everything after \"ANN=\"\n",
    "    ann_content = info_field[ann_start + 4:]\n",
    "    \n",
    "    # If there are other fields after ANN, cut them off\n",
    "    if ';' in ann_content:\n",
    "        ann_content = ann_content.split(';')[0]\n",
    "    \n",
    "    # Split by comma to get individual annotations\n",
    "    annotations = ann_content.split(',')\n",
    "    \n",
    "    # Get the first annotation\n",
    "    first_ann = annotations[0]\n",
    "    \n",
    "    # Split by pipe (|) to get annotation fields\n",
    "    ann_fields = first_ann.split('|')\n",
    "    \n",
    "    # Create result dictionary\n",
    "    # Standard VCF ANN format: Allele | Annotation | Impact | Gene_Name | ...\n",
    "    if len(ann_fields) >= 4:\n",
    "        result = {\n",
    "            'allele': ann_fields[0],\n",
    "            'type': ann_fields[1],\n",
    "            'impact': ann_fields[2],\n",
    "            'gene_id': ann_fields[3]\n",
    "        }\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def add_ann_info_to_df(df, info_column='INFO'):\n",
    "    \"\"\"\n",
    "    Extract the first ANN annotation from the INFO field and add as separate columns to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing VCF data with an INFO column\n",
    "    info_column : str, default='INFO'\n",
    "        Name of the column containing the INFO field\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The original DataFrame with additional columns for variant type, impact, and gene ID\n",
    "    \"\"\"\n",
    "    # Create new columns with None values\n",
    "    df['variant_type'] = None\n",
    "    df['impact'] = None\n",
    "    df['gene_id'] = None\n",
    "    df['allele'] = None\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        info = row[info_column]\n",
    "        ann_data = extract_first_ann(info)\n",
    "        \n",
    "        if ann_data:\n",
    "            df.loc[idx, 'variant_type'] = ann_data['type']\n",
    "            df.loc[idx, 'impact'] = ann_data['impact']\n",
    "            df.loc[idx, 'gene_id'] = ann_data['gene_id']\n",
    "            df.loc[idx, 'allele'] = ann_data['allele']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a DataFrame 'vcf_df' with a column 'INFO' containing VCF INFO fields\n",
    "# vcf_df = add_ann_info_to_df(vcf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'allele': 'G', 'type': 'synonymous_variant', 'impact': 'LOW', 'gene_id': 'cgd1_340'}\n"
     ]
    }
   ],
   "source": [
    "# Test with the example\n",
    "info_field = 'AB=0;ABP=0;AC=1;AF=0.142857;AN=7;AO=248;CIGAR=1X;DP=664;DPB=664;DPRA=0;EPP=13.1322;EPPR=3.03118;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=7;NUMALT=1;ODDS=61.2192;PAIRED=0.991935;PAIREDR=0.995192;PAO=0;PQA=0;PQR=0;PRO=0;QA=8364;QR=13932;RO=416;RPL=137;RPP=8.92931;RPPR=9.04449;RPR=111;RUN=1;SAF=122;SAP=3.15039;SAR=126;SRF=210;SRP=3.09382;SRR=206;TYPE=snp;technology.illumina=1;ANN=G|synonymous_variant|LOW|cgd1_340|cgd1_340|transcript|cgd1_340-RA|protein_coding|1/1|c.357A>G|p.Ser119Ser|570/1483|357/1194|119/397||,G|upstream_gene_variant|MODIFIER|cgd1_320|cgd1_320|transcript|cgd1_320-RA|protein_coding||c.-4699T>C|||||4675|,G|upstream_gene_variant|MODIFIER|cgd1_330|cgd1_330|transcript|cgd1_330-RA|protein_coding||c.-1174T>C|||||1095|,G|upstream_gene_variant|MODIFIER|cgd1_350|cgd1_350|transcript|cgd1_350-RA|protein_coding||c.-1490A>G|||||1323|,G|downstream_gene_variant|MODIFIER|cgd1_360|cgd1_360|transcript|cgd1_360-RA|protein_coding||c.*4665T>C|||||4665|'\n",
    "result = extract_first_ann(info_field)\n",
    "print(result)\n",
    "# Should output: {'allele': 'G', 'type': 'synonymous_variant', 'impact': 'LOW', 'gene_id': 'cgd1_340'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def mod_hist_legend(ax, title=False):\n",
    "    \"\"\"\n",
    "    Creates a cleaner legend for histogram plots by using line elements instead of patches.\n",
    "    \n",
    "    Motivation:\n",
    "    - Default histogram legends show rectangle patches which can be visually distracting\n",
    "    - This function creates a more elegant legend with simple lines matching histogram edge colors\n",
    "    - Positions the legend outside the plot to avoid overlapping with data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object containing the histogram(s)\n",
    "    title : str or bool, default=False\n",
    "        Optional title for the legend. If False, no title is displayed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None - modifies the axes object in place\n",
    "    \"\"\"\n",
    "    # Extract the current handles and labels from the plot\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    # Create new line handles that match the edge colors of histogram bars\n",
    "    # This produces a cleaner, more minimal legend appearance\n",
    "    new_handles = [matplotlib.lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
    "    \n",
    "    # Create the legend with custom positioning\n",
    "    # - Places legend outside the plot (to the right) to avoid obscuring the data\n",
    "    # - Centers the legend vertically for better visual balance\n",
    "    ax.legend(handles=new_handles, \n",
    "              labels=labels, \n",
    "              title=title,\n",
    "              loc='center left', \n",
    "              bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "\n",
    "def clean_axes(ax, offset=10):\n",
    "    \"\"\"\n",
    "    Customizes a matplotlib axes by removing top and right spines,\n",
    "    and creating a broken axis effect where x and y axes don't touch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to customize\n",
    "    offset : int, default=10\n",
    "        The amount of offset/gap between the x and y axes in points\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The same axes object, modified in place\n",
    "    \"\"\"\n",
    "    # Remove the top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Make the remaining spines gray for a more subtle look\n",
    "    ax.spines['left'].set_color('gray')\n",
    "    ax.spines['bottom'].set_color('gray')\n",
    "    \n",
    "    # Create the broken axis effect\n",
    "    # Move the bottom spine up by offset points\n",
    "    #ax.spines['bottom'].set_position(('outward', offset))\n",
    "    \n",
    "    # Move the left spine right by offset points\n",
    "    ax.spines['left'].set_position(('outward', offset))\n",
    "    \n",
    "    # Return the modified axes\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def make_circos_plot(data):\n",
    "    \n",
    "    seqid2size = {\n",
    "        'CM000429': 875659,\n",
    "        'CM000430': 985969,\n",
    "        'CM000431': 1099352,\n",
    "        'CM000432': 1104417,\n",
    "        'CM000433': 1080900,\n",
    "        'CM000434': 1332857,\n",
    "        'CM000435': 1278458,\n",
    "        'CM000436': 1344712\n",
    "    }\n",
    "\n",
    "    color_dict = {'M': 'green', 'C': 'blue'}\n",
    "\n",
    "    circos = Circos(seqid2size, space=3, start=-83, end=265, endspace=False)\n",
    "    circos.text(\"C. parvum IowaII\", r=5, size=18, font={'style': 'italic'})\n",
    "    \n",
    "    m_samples = ['AF_M7', 'AF_M5', 'AF_M6', 'AF_M4']\n",
    "    c_samples = ['AF_C3', 'AF_C2', 'AF_C1']\n",
    "    \n",
    "    for sector in circos.sectors:\n",
    "        sector.text(sector.name[-3:])\n",
    "        \n",
    "        m_track = sector.add_track((80, 100))\n",
    "        m_track.xticks_by_interval(200000, show_label=False)\n",
    "        m_track.axis()\n",
    "    \n",
    "        \n",
    "        c_track = sector.add_track((55, 75))\n",
    "        c_track.xticks_by_interval(200000, show_label=False)\n",
    "        c_track.axis()\n",
    "        \n",
    "        # Plot scatter points for each sample group\n",
    "        for sample in m_samples:\n",
    "            color = color_dict[sample[3]]\n",
    "            subset = data[(data['#CHROM'] == sector.name) & (data[sample] > 0)]\n",
    "            m_track.scatter(\n",
    "                x=subset['POS'].values,\n",
    "                y=subset[sample].values,\n",
    "                c=color,\n",
    "                s=3,\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                alpha=0.3,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        for sample in c_samples:\n",
    "            color = color_dict[sample[3]]\n",
    "            subset = data[(data['#CHROM'] == sector.name) & (data[sample] > 0)]\n",
    "            c_track.scatter(\n",
    "                x=subset['POS'].values,\n",
    "                y=subset[sample].values,\n",
    "                c=color,\n",
    "                s=3,\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                alpha=0.3,\n",
    "            )\n",
    "\n",
    "        \n",
    "        # Optional: Add labels \n",
    "        if sector.name == 'CM000429':\n",
    "            m_track.yticks([0, 1], [\"0\", \"1\"], side=\"left\")\n",
    "            c_track.yticks([0, 1], [\"0\", \"1\"], side=\"left\")\n",
    "\n",
    "            \n",
    "            circos.text(\"Cow\", r=c_track.r_center, deg=-90, color=\"blue\")\n",
    "            circos.text(\"Mouse\", r=m_track.r_center, deg=-90, color=\"green\")\n",
    "    \n",
    "    circos.plotfig()\n",
    "    circos.savefig('../data/Circos.svg')\n",
    "    circos.savefig('../data/Circos.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def kmeans_cluster_analysis(df, cluster_sizes, random_state=42, features=None, figsize=(12, 6), \n",
    "                          standardize=False, fill_na=False):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering analysis on a pandas DataFrame and visualize the results\n",
    "    with both normalized inertia and silhouette scores on the same plot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input data to cluster.\n",
    "    cluster_sizes : list\n",
    "        List of cluster sizes (k values) to evaluate.\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility (default: 42).\n",
    "    features : list, optional\n",
    "        List of column names to use for clustering. If None, all columns are used.\n",
    "    figsize : tuple, optional\n",
    "        Figure size for the output plot (default: (12, 6)).\n",
    "    standardize : bool, optional\n",
    "        Whether to standardize the features (default: False).\n",
    "    fill_na : bool, optional\n",
    "        Whether to fill missing values with column means (default: False).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (figure, inertia_values, silhouette_values) - The matplotlib figure object,\n",
    "        the list of inertia values, and the list of silhouette scores.\n",
    "    \"\"\"\n",
    "    # Prepare the data\n",
    "    if features is None:\n",
    "        features = df.columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # Check for non-numeric data\n",
    "    non_numeric_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "    if non_numeric_cols:\n",
    "        raise ValueError(f\"Non-numeric columns found: {non_numeric_cols}. \"\n",
    "                         f\"Please remove or transform them before clustering.\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    if X.isna().any().any():\n",
    "        if fill_na:\n",
    "            print(\"Filling missing values with column means.\")\n",
    "            X = X.fillna(X.mean())\n",
    "        else:\n",
    "            raise ValueError(\"Missing values found in the data. Set fill_na=True to automatically handle them or preprocess your data before clustering.\")\n",
    "    \n",
    "    # Prepare data for clustering\n",
    "    if standardize:\n",
    "        print(\"Standardizing features.\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = X.values\n",
    "    \n",
    "    # Compute K-means for different cluster sizes\n",
    "    inertia_values = []\n",
    "    silhouette_values = []\n",
    "    \n",
    "    for k in cluster_sizes:\n",
    "        # Fit K-means\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia_values.append(kmeans.inertia_)\n",
    "        \n",
    "        # Compute silhouette score (not defined for k=1)\n",
    "        if k > 1:\n",
    "            labels = kmeans.labels_\n",
    "            silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "            silhouette_values.append(silhouette_avg)\n",
    "        else:\n",
    "            silhouette_values.append(0)  # Placeholder for k=1\n",
    "    \n",
    "    # Normalize values\n",
    "    max_inertia = max(inertia_values)\n",
    "    normalized_inertia = [i / max_inertia for i in inertia_values]\n",
    "    \n",
    "    max_silhouette = max(silhouette_values)\n",
    "    normalized_silhouette = [s / max_silhouette for s in silhouette_values]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot normalized inertia (elbow curve)\n",
    "    inertia_line, = ax.plot(cluster_sizes, normalized_inertia, 'o-', color='blue', label='Normalized Inertia')\n",
    "    \n",
    "    # Plot normalized silhouette scores\n",
    "    silhouette_line, = ax.plot(cluster_sizes, normalized_silhouette, 'o-', color='red', label='Normalized Silhouette Score')\n",
    "    \n",
    "    # Add vertical lines at each cluster size\n",
    "    for k in cluster_sizes:\n",
    "        ax.axvline(x=k, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title('K-means Evaluation', fontsize=15)\n",
    "    ax.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "    ax.set_ylabel('Normalized Score', fontsize=12)\n",
    "    ax.set_xticks(cluster_sizes)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Find optimal k values\n",
    "    best_inertia_idx = elbow_point(normalized_inertia)\n",
    "    best_silhouette_idx = np.argmax(normalized_silhouette)\n",
    "    \n",
    "    best_k_inertia = cluster_sizes[best_inertia_idx]\n",
    "    best_k_silhouette = cluster_sizes[best_silhouette_idx]\n",
    "    \n",
    "    # Add arrows to optimal points without text\n",
    "    inertia_arrow = ax.annotate('', \n",
    "                xy=(best_k_inertia, normalized_inertia[best_inertia_idx]),\n",
    "                xytext=(best_k_inertia+0.5, normalized_inertia[best_inertia_idx]+0.1),\n",
    "                arrowprops=dict(facecolor='blue', shrink=0.05, width=1.5, headwidth=8))\n",
    "    \n",
    "    silhouette_arrow = ax.annotate('', \n",
    "                xy=(best_k_silhouette, normalized_silhouette[best_silhouette_idx]),\n",
    "                xytext=(best_k_silhouette+0.5, normalized_silhouette[best_silhouette_idx]-0.1),\n",
    "                arrowprops=dict(facecolor='red', shrink=0.05, width=1.5, headwidth=8))\n",
    "    \n",
    "    # Create custom legend handles for the arrows\n",
    "    from matplotlib.lines import Line2D\n",
    "    \n",
    "    elbow_arrow_handle = Line2D([0], [0], color='blue', marker='>',\n",
    "                              markersize=10, linestyle='-', linewidth=0)\n",
    "    silhouette_arrow_handle = Line2D([0], [0], color='red', marker='>',\n",
    "                                   markersize=10, linestyle='-', linewidth=0)\n",
    "    \n",
    "    # Create a legend with the arrows and lines\n",
    "    legend_elements = [\n",
    "        inertia_line, silhouette_line,\n",
    "        elbow_arrow_handle, silhouette_arrow_handle\n",
    "    ]\n",
    "    legend_labels = [\n",
    "        'Normalized Inertia', 'Normalized Silhouette Score',\n",
    "        f'Best Elbow (k={best_k_inertia})', f'Best Silhouette (k={best_k_silhouette})'\n",
    "    ]\n",
    "    \n",
    "    # Place the legend outside the plot\n",
    "    ax.legend(legend_elements, legend_labels, loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "    \n",
    "    # Adjust layout to make room for the legend\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(right=0.75)\n",
    "    \n",
    "    # Return the figure, axis, and values to allow further customization\n",
    "    return fig, ax, inertia_values, silhouette_values\n",
    "\n",
    "def elbow_point(values):\n",
    "    \"\"\"\n",
    "    Find the elbow point in a curve using the maximum curvature method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    values : list\n",
    "        The y-values of the curve.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the elbow point.\n",
    "    \"\"\"\n",
    "    # Simple method - find point of maximum curvature\n",
    "    # Convert to numpy array\n",
    "    y = np.array(values)\n",
    "    x = np.arange(len(y))\n",
    "    \n",
    "    # Compute first and second derivatives\n",
    "    dy = np.gradient(y)\n",
    "    d2y = np.gradient(dy)\n",
    "    \n",
    "    # Compute curvature\n",
    "    curvature = np.abs(d2y) / (1 + dy**2)**1.5\n",
    "    \n",
    "    # Return the point of maximum curvature (ignoring the first and last points)\n",
    "    if len(curvature) <= 2:\n",
    "        return 0\n",
    "    return np.argmax(curvature[1:-1]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def analyze_frequency_distribution(frequency_data, n_iter=100, min_k=1, max_k=5):\n",
    "    \"\"\"Fit GMMs with different components and compare AIC scores.\"\"\"\n",
    "    # Prepare data for GMM (must be 2D array)\n",
    "    X = frequency_data.values.reshape(-1, 1)\n",
    "    \n",
    "    # Store results\n",
    "    aic_results = {k: [] for k in range(min_k, max_k+1)}\n",
    "    \n",
    "    # Fit models with different k values\n",
    "    for k in range(min_k, max_k+1):\n",
    "        print(f\"Fitting models with k={k}\")\n",
    "        for i in range(n_iter):\n",
    "            gmm = GaussianMixture(n_components=k, random_state=i)\n",
    "            gmm.fit(X)\n",
    "            aic_results[k].append(gmm.aic(X))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    aic_means = [np.mean(aic_results[k]) for k in range(min_k, max_k+1)]\n",
    "    aic_stds = [np.std(aic_results[k]) for k in range(min_k, max_k+1)]\n",
    "    k_values = list(range(min_k, max_k+1))\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(k_values, aic_means, yerr=aic_stds, marker='o', capsize=6, linestyle='-', linewidth=2)\n",
    "    plt.xlabel('Number of Components (k)', fontsize=12)\n",
    "    plt.ylabel('AIC Score (Mean ± SD)', fontsize=12)\n",
    "    plt.title('AIC Scores for GMM with Different Numbers of Components', fontsize=14)\n",
    "    plt.xticks(k_values)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find best k based on lowest mean AIC\n",
    "    best_k = k_values[np.argmin(aic_means)]\n",
    "    plt.annotate(f'Best k = {best_k}', \n",
    "                xy=(best_k, min(aic_means)), \n",
    "                xytext=(best_k, min(aic_means) - 0.05 * (max(aic_means) - min(aic_means))),\n",
    "                ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    return k_values, aic_means, aic_stds, best_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_chromosomal_af_values(df):\n",
    "    # Get list of all AF columns\n",
    "    af_columns = [col for col in df.columns if col.startswith('AF_')]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Get unique chromosomes and assign colors to AF columns\n",
    "    chromosomes = df['#CHROM'].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(af_columns)))\n",
    "    \n",
    "    # Track chromosome boundaries for labeling\n",
    "    chrom_boundaries = {}\n",
    "    current_x = 0\n",
    "    margin = 1000  # Margin to prevent overlap with y-axis\n",
    "    current_x += margin\n",
    "    \n",
    "    # Process each chromosome\n",
    "    for chrom in chromosomes:\n",
    "        # Get data for this chromosome\n",
    "        chrom_data = df[df['#CHROM'] == chrom]\n",
    "        \n",
    "        # Sort by position\n",
    "        chrom_data = chrom_data.sort_values('POS')\n",
    "        \n",
    "        # Get minimum position for this chromosome\n",
    "        min_pos = chrom_data['POS'].min()\n",
    "        \n",
    "        # Store starting x-coordinate for this chromosome\n",
    "        chrom_start = current_x\n",
    "        \n",
    "        # Plot each AF column for this chromosome\n",
    "        for i, af_col in enumerate(af_columns):\n",
    "            # Get x-coordinates (adjusted positions)\n",
    "            x = chrom_data['POS'] - min_pos + current_x\n",
    "            y = chrom_data[af_col]\n",
    "            \n",
    "            # Plot scatter points only (no lines)\n",
    "            ax.scatter(x, y, color=colors[i], alpha=0.7, s=5, \n",
    "                      label=af_col if chrom == chromosomes[0] else \"\")\n",
    "        \n",
    "        # Update current_x for next chromosome\n",
    "        max_pos = chrom_data['POS'].max()\n",
    "        current_x = current_x + (max_pos - min_pos) + 1000  # Add gap between chromosomes\n",
    "        \n",
    "        # Store ending boundary\n",
    "        chrom_boundaries[chrom] = (chrom_start, current_x - 1000)\n",
    "    \n",
    "    # Add chromosome boxes and labels\n",
    "    y_box_position = -0.05\n",
    "    box_height = 0.02\n",
    "    \n",
    "    for chrom, (start, end) in chrom_boundaries.items():\n",
    "        # Get short chromosome name (last 3 digits)\n",
    "        short_name = str(chrom)[-3:] if len(str(chrom)) > 3 else str(chrom)\n",
    "        \n",
    "        # Add box - make them more visible\n",
    "        center = (start + end) / 2\n",
    "        width = end - start\n",
    "        rect = plt.Rectangle((start, y_box_position), width, box_height, \n",
    "                           facecolor='gray', edgecolor='black', linewidth=1.5,\n",
    "                           transform=ax.transData)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label below the box\n",
    "        plt.text(center, y_box_position - 0.03, short_name, ha='center', va='top', \n",
    "                fontsize=10, fontweight='bold', transform=ax.transData)\n",
    "    \n",
    "    # Set plot limits and labels\n",
    "    ax.set_ylim(-0.1, 1.05)  # Extend lower limit to see boxes clearly\n",
    "    ax.set_xlim(0, current_x)\n",
    "    ax.set_ylabel('Allele Frequency', fontsize=12)\n",
    "    ax.set_title('Allele Frequencies Across Chromosomes', fontsize=14)\n",
    "    \n",
    "    # Remove all axes except left y-axis\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    xlim = ax.get_xlim()\n",
    "    #ax.set_xlim(xlim[0]-1000,xlim[1]+100)\n",
    "    # Remove x ticks\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "    # Add legend outside plot\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def analyze_frequency_distribution(frequency_data, n_iter=10, min_k=1, max_k=4):\n",
    "    import numpy as np\n",
    "    \"\"\"Fit GMMs with different components and compare AIC scores.\"\"\"\n",
    "    # Prepare data for GMM (must be 2D array)\n",
    "    X = frequency_data.values.reshape(-1, 1)\n",
    "    \n",
    "    # Store results\n",
    "    aic_results = {k: [] for k in range(min_k, max_k+1)}\n",
    "    best_models = {}\n",
    "    \n",
    "    # Fit models with different k values\n",
    "    for k in range(min_k, max_k+1):\n",
    "        print(f\"Fitting models with k={k}\")\n",
    "        best_aic = np.inf\n",
    "        best_model = None\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            gmm = GaussianMixture(n_components=k, random_state=i)\n",
    "            gmm.fit(X)\n",
    "            aic = gmm.aic(X)\n",
    "            aic_results[k].append(aic)\n",
    "            \n",
    "            # Keep track of best model for each k\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_model = gmm\n",
    "        \n",
    "        best_models[k] = best_model\n",
    "    \n",
    "    # Calculate statistics\n",
    "    aic_means = [np.mean(aic_results[k]) for k in range(min_k, max_k+1)]\n",
    "    aic_stds = [np.std(aic_results[k]) for k in range(min_k, max_k+1)]\n",
    "    k_values = list(range(min_k, max_k+1))\n",
    "    \n",
    "    # Find best k based on lowest mean AIC\n",
    "    best_k = k_values[np.argmin(aic_means)]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "    \n",
    "    # Plot 1: AIC scores\n",
    "    ax1.errorbar(k_values, aic_means, yerr=aic_stds, xerr=None, marker='o', capsize=6, linestyle='-', linewidth=2)\n",
    "    ax1.set_xlabel('Number of Components (k)', fontsize=12)\n",
    "    ax1.set_ylabel('AIC Score (Mean ± SD)', fontsize=12)\n",
    "    ax1.set_title('AIC Scores for GMM with Different Numbers of Components', fontsize=14)\n",
    "    ax1.set_xticks(k_values)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.annotate(f'Best k = {best_k}', \n",
    "                xy=(best_k, min(aic_means)), \n",
    "                xytext=(best_k, min(aic_means) - 0.05 * (max(aic_means) - min(aic_means))),\n",
    "                ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Data KDE and components of best model\n",
    "    best_gmm = best_models[best_k]\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    # Plot KDE of original data (normalized, no histogram)\n",
    "    sns.kdeplot(frequency_data, ax=ax2, color=\"gray\", label=\"Data KDE\")\n",
    "    \n",
    "    # Plot GMM components\n",
    "    y = np.zeros_like(x)\n",
    "    for i in range(best_k):\n",
    "        mean = best_gmm.means_[i, 0]\n",
    "        std = np.sqrt(best_gmm.covariances_[i, 0, 0])\n",
    "        weight = best_gmm.weights_[i]\n",
    "        \n",
    "        # Plot individual component\n",
    "        component = weight * norm.pdf(x, mean, std)\n",
    "        #ax2.plot(x, component, label=f'Component {i+1}: μ={mean:.2f}, w={weight:.2f}')\n",
    "        ax2.plot(x, component, label=f'CMP {i+1}')\n",
    "        # Add to mixture density\n",
    "        y += component\n",
    "    \n",
    "    # Plot overall density\n",
    "    ax2.plot(x, y, 'k-', linewidth=2, label='GMM')\n",
    "    \n",
    "    ax2.set_xlabel('Frequency', fontsize=12)\n",
    "    ax2.set_ylabel('Density', fontsize=12)\n",
    "    ax2.set_title(f'Data Distribution and GMM Components (k={best_k})', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return k_values, aic_means, aic_stds, best_k, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
